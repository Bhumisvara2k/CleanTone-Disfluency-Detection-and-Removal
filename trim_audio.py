# -*- coding: utf-8 -*-
"""Trim_Audio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EG48oYE11IWjOh7zQHFznpDucQmaDN9A

# CleanTone: Enhancing the Audio Clarity through advanced disfluency detection and Removal

# Mounts drive and install libraries
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install whisper_timestamped

import whisper_timestamped as whisper
import librosa
import librosa.display
import IPython.display as ipd
import json
import os
ROOT_DIR = os.getcwd()
f=0

"""# Record audio from your microphone"""

!pip install ffmpeg-python

import os

folder_path = '/content/drive/My Drive/Live_Record'

files = os.listdir(folder_path)
for file in files:
    file_path = os.path.join(folder_path, file)
    os.remove(file_path)

from IPython.display import HTML
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from scipy.io.wavfile import read as wav_read
import io
import ffmpeg
import os

# HTML template for recording audio
AUDIO_HTML = """
<script>
var my_div = document.createElement("DIV");
var my_p = document.createElement("P");
var my_btn = document.createElement("BUTTON");
var t = document.createTextNode("Press to start recording");

my_btn.appendChild(t);
//my_p.appendChild(my_btn);
my_div.appendChild(my_btn);
document.body.appendChild(my_div);

var base64data = 0;
var reader;
var recorder, gumStream;
var recordButton = my_btn;

var handleSuccess = function(stream) {
  gumStream = stream;
  var options = {
    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k
    mimeType : 'audio/webm;codecs=opus'
    //mimeType : 'audio/webm;codecs=pcm'
  };
  //recorder = new MediaRecorder(stream, options);
  recorder = new MediaRecorder(stream);
  recorder.ondataavailable = function(e) {
    var url = URL.createObjectURL(e.data);
    var preview = document.createElement('audio');
    preview.controls = true;
    preview.src = url;
    document.body.appendChild(preview);

    reader = new FileReader();
    reader.readAsDataURL(e.data);
    reader.onloadend = function() {
      base64data = reader.result;
      //console.log("Inside FileReader:" + base64data);
    }
  };
  recorder.start();
  };

recordButton.innerText = "Recording... press to stop";

navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);


function toggleRecording() {
  if (recorder && recorder.state == "recording") {
      recorder.stop();
      gumStream.getAudioTracks()[0].stop();
      recordButton.innerText = "Saving the recording... pls wait!"
  }
}

// https://stackoverflow.com/a/951057
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

var data = new Promise(resolve=>{
//recordButton.addEventListener("click", toggleRecording);
recordButton.onclick = ()=>{
toggleRecording()

sleep(2000).then(() => {
  // wait 2000ms for the data to be available...
  // ideally this should use something like await...
  //console.log("Inside data:" + base64data)
  resolve(base64data.toString())

});

}
});

</script>
"""

# Function to save audio as MP3 file
def save_audio(audio_data, output_file):
    binary_data = b64decode(audio_data.split(',')[1])
    with open(output_file, 'wb') as f:
        f.write(binary_data)

# Function to convert WAV to MP3
def convert_to_mp3(wav_file, mp3_file):
    process = (
        ffmpeg.input(wav_file)
        .output(mp3_file, codec='libmp3lame')
        .run_async(overwrite_output=True)
    )
    process.communicate()

# Create "Live_Record" folder if it doesn't exist
f=0
output_folder = "/content/drive/My Drive/Live_Record"
folder = "/content/drive/My Drive/Output_Files"
os.makedirs(output_folder, exist_ok=True)

# Display the HTML for recording audio
display(HTML(AUDIO_HTML))

# Record audio
try:
    data = eval_js("data")
except Exception as e:
    print("An error occurred while trying to evaluate JavaScript code:", e)
    data = None

if data:
    binary = b64decode(data.split(',')[1])

    # Write audio to a WAV file
    wav_file = os.path.join(folder, "recorded_audio.wav")
    try:
        with open(wav_file, 'wb') as f:
            f.write(binary)
    except Exception as e:
        print("Error saving WAV file:", e)
        wav_file = None

    if wav_file:
        # Convert WAV to MP3
        mp3_file = os.path.join(output_folder, "recorded_audio.mp3")
        try:
            convert_to_mp3(wav_file, mp3_file)
            print("Audio recording saved as MP3 file:", mp3_file)
            f=1
        except Exception as e:
            print("Error converting WAV to MP3:", e)
    else:
        print("No WAV file saved, skipping MP3 conversion.")
else:
    print("No audio data received.")

"""# Loading the audio files"""

# Path to the directory containing audio files
if f==1:
  audio_dir = '/content/drive/My Drive/Live_Record'
else:
  audio_dir = '/content/drive/My Drive/Sample'

"""Loading Audio Files from drive"""

import os
import librosa
import IPython.display as ipd

# List all files in the directory
audio_files = os.listdir(audio_dir)

# Load each audio file
for file in audio_files:
    file_path = os.path.join(audio_dir, file)
    audio_data, sample_rate = librosa.load(file_path)

"""# Feature Extraction Module

MFCC Features can be extracted and stored in features.npy
"""

import librosa
import os
import numpy as np
import matplotlib.pyplot as plt

# Initialize lists to store features
features = []

max_len = 100

# Loop through each file in the folder
for filename in os.listdir(audio_dir):
    if filename.endswith('.mp3'):
        file_path = os.path.join(audio_dir, filename)
        y, sr = librosa.load(file_path, sr=None)

        # Extract MFCC features
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        print(f"File: {filename}, MFCC shape: {mfccs.shape}")

        # Pad or truncate the MFCC features to ensure a consistent length
        if mfccs.shape[1] < max_len:
            pad_width = max_len - mfccs.shape[1]
            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')
        else:
            mfccs = mfccs[:, :max_len]
        features.append(mfccs)

features = np.array(features)
np.save('features.npy', features)
print("Features shape:", features.shape)

"""# Disfluency Detection Module

Training CNN Model and Disfluency Detection
"""

import os
import whisper_timestamped as whisper
import warnings
import matplotlib.pyplot as plt

def predict_disfluency_type(word):
    if "umm" in word.lower():
        return "umm"
    elif "uhh" in word.lower():
        return "uhh"
    else:
        return "disfluent"

# Load speech recognition model
model = whisper.load_model("tiny", device="cpu")
audio_files_info = []

for file in os.listdir(audio_dir):
    file_path = os.path.join(audio_dir, file)
    audio = whisper.load_audio(file_path)

    warnings.filterwarnings("ignore", category=UserWarning, module="whisper")
    result = whisper.transcribe(model, audio, language="en", detect_disfluencies=True)
    print(result)

    transcription = ""
    disfluency_count = 0
    disfluency_segments = []

    # Print the transcription for the current file
    print(f"Transcription for {file}:")
    for segment in result["segments"]:
       for word in segment["words"]:
          if "[*]" in word["text"]:
             disfluency_type = predict_disfluency_type(word["text"])
             print(f"[{disfluency_type}] ", end='')
             disfluency_count += 1

             disfluency_segments.append({
                "start_time": word["start"],
                "end_time": word["end"],
                "text": word["text"]
             })
          else:
              print(word["text"], end=' ')
          transcription += word["text"] + " "
       #print()

    audio_files_info.append({
    "file_name": file,
    "transcription": transcription,
    "disfluency_count": disfluency_count,
    "disfluency_segments": disfluency_segments
    })

    print(f"\nNumber of disfluencies detected: {disfluency_count}")
    print()

"""Labeling Audio Dataset (Disfluent/Fluent)"""

import os
import librosa
import numpy as np

# Initialize lists to store labels
labels = []

max_len = 100

for audio_info in audio_files_info:
     dis_cnt = audio_info["disfluency_count"]
        # Determine label based on disfluency count
     label = 1 if dis_cnt == 0 else 0
     labels.append(label)

labels = np.array(labels)
np.save('labels.npy', labels)

"""Training the model with features and labels"""

import numpy as np
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from sklearn.model_selection import train_test_split

# Load your features and labels
features = np.load('features.npy')
labels = np.load('labels.npy')

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)

# Define CNN model architecture
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(13, 100, 1)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_val, y_val)
print('Test Loss:', test_loss)
print('Test Accuracy:', test_accuracy)

"""Displaying the Disfluencies count in each file"""

import matplotlib.pyplot as plt

# Extract file names and disfluency counts for plotting
file_names = [info["file_name"] for info in audio_files_info]
disfluency_counts = [info["disfluency_count"] for info in audio_files_info]

# Create a figure and axis object
fig, ax = plt.subplots(figsize=(12, 4))

# Plot the bar graph
ax.bar(file_names, disfluency_counts, color='skyblue', alpha=0.7)

# Add labels and title
ax.set_xlabel('Audio Files')
ax.set_ylabel('Number of Disfluencies')
ax.set_title('Number of Disfluencies in Each Audio File')

# Rotate x-axis labels for better readability (optional)
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

"""# Disfluency Removal Module

Installing the moviepy library
"""

pip install moviepy

"""Cleaning the folders Output_Files and Merged_Audio"""

import os

folder_path = '/content/drive/My Drive/Output_Files'

files = os.listdir(folder_path)
for file in files:
    file_path = os.path.join(folder_path, file)
    os.remove(file_path)

import os

folder_path = '/content/drive/My Drive/Merged_Audio'

files = os.listdir(folder_path)
for file in files:
    file_path = os.path.join(folder_path, file)
    os.remove(file_path)

"""Trimming the audio files"""

import os
from moviepy.editor import AudioFileClip

def trim_and_save_audio(audio_info, output_folder):
    try:
        file_name = audio_info["file_name"]
        disfluency_segments = audio_info.get("disfluency_segments", [])

        # Load the original audio file
        input_file_path = os.path.join(audio_dir, file_name)
        audio_clip = AudioFileClip(input_file_path)
        start_time = 0
        end_time = None

        for i, segment in enumerate(disfluency_segments):
            segment_start_time = segment["start_time"]
            segment_end_time = segment["end_time"]

            trimmed_clip = audio_clip.subclip(start_time, segment_start_time)

            output_file_name = f"{file_name}_{i+1}.mp3"
            output_file_path = os.path.join(output_folder, output_file_name)

            trimmed_clip.write_audiofile(output_file_path)

            start_time = segment_end_time

        if disfluency_segments:
            last_segment_end_time = disfluency_segments[-1]["end_time"]
            trimmed_clip = audio_clip.subclip(last_segment_end_time)
            output_file_name = f"{file_name}_{len(disfluency_segments) + 1}.mp3"
            output_file_path = os.path.join(output_folder, output_file_name)
            trimmed_clip.write_audiofile(output_file_path)
        else:
            output_file_path = os.path.join(output_folder, file_name)
            audio_clip.write_audiofile(output_file_path)

        audio_clip.close()

        print(f"Stored: {file_name}")

    except KeyError as e:
        print(f"Error: {e}. 'segments' key not found in audio_info.")


output_folder = "/content/drive/My Drive/Output_Files"

for audio_info in audio_files_info:
    trim_and_save_audio(audio_info, output_folder)

"""Concatenate the trimmed audio files (Small Chunks)"""

import os
from moviepy.editor import concatenate_audioclips, AudioFileClip

def concatenate_and_save(input_folder, output_folder, increase_volume=True, remove_noise=True):
    audio_files = os.listdir(input_folder)

    audio_clips_dict = {}

    for file_name in audio_files:
        file_prefix = file_name.split('.')[0]

        trimmed_file_path = os.path.join(input_folder, file_name)

        audio_clip = AudioFileClip(trimmed_file_path)

        if increase_volume:
            audio_clip = audio_clip.volumex(1.5)

        if file_prefix in audio_clips_dict:
            audio_clips_dict[file_prefix].append(audio_clip)
        else:
            audio_clips_dict[file_prefix] = [audio_clip]

    # Concatenate and save the audio clips for each original audio file
    for file_prefix, audio_clips in audio_clips_dict.items():
        merged_clip = concatenate_audioclips(audio_clips)
        output_file_path = os.path.join(output_folder, f"{file_prefix}_merged.mp3")
        merged_clip.write_audiofile(output_file_path, codec='mp3')
        merged_clip.close()

input_folder = "/content/drive/My Drive/Output_Files"
output_folder = "/content/drive/My Drive/Merged_Audio"

os.makedirs(output_folder, exist_ok=True)

concatenate_and_save(input_folder, output_folder)

"""# Display and Load the merged output audio

Source Input Files
"""

from IPython.display import Audio, display
import os
output_folder = "/content/drive/My Drive/Sample"
# List all files in the folder
audio_files = os.listdir(output_folder)

for audio_file in audio_files:
    if audio_file.endswith('.wav') or audio_file.endswith('.mp3'):
        print("Audio File:", audio_file)

        # Display and play the audio file
        display(Audio(filename=os.path.join(output_folder, audio_file), autoplay=False))

"""Processed Output Files"""

from IPython.display import Audio, display
import os
output_folder = "/content/drive/My Drive/Merged_Audio"
# List all files in the folder
audio_files = os.listdir(output_folder)

for audio_file in audio_files:
    if audio_file.endswith('.wav') or audio_file.endswith('.mp3'):
        print("Audio File:", audio_file)

        # Display and play the audio file
        display(Audio(filename=os.path.join(output_folder, audio_file), autoplay=False))

"""# Questions Generation Module"""

pip install google-generativeai

import google.generativeai as genai

genai.configure(api_key="AIzaSyAe40M1eoL-zNvGleh9Lj9XMcmSnDlQlac")

# Set up the model
generation_config = {
  "temperature": 1,
  "top_p": 0.95,
  "top_k": 0,
  "max_output_tokens": 8192,
}

safety_settings = [
  {
    "category": "HARM_CATEGORY_HARASSMENT",
    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
  },
  {
    "category": "HARM_CATEGORY_HATE_SPEECH",
    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
  },
  {
    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
  },
  {
    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
  },
]

model = genai.GenerativeModel(model_name="gemini-1.5-pro-latest",
                              generation_config=generation_config,
                              safety_settings=safety_settings)

question_count = 0
for audio_info in audio_files_info:
    file_name = audio_info["file_name"]
    transcription = audio_info["transcription"]

    prompt = """Generate five questions based on the given text.

    Text:
    {}
    """.format(transcription)

    convo = model.start_chat(history=[])
    convo.send_message(prompt)

    print(f"Questions for {file_name}:")
    message = convo.last
    if message:
        question = message.text
        print(f"{question}")
        question_count += 1
    else:
        print("No response received")

    if question_count >= 5:
        break

"""# Performance Analysis Module"""

import os
import librosa
import numpy as np
import warnings
import matplotlib.pyplot as plt

input_dir = "/content/drive/My Drive/Sample"
output_dir = "/content/drive/My Drive/Merged_Audio"
input_files = [file for file in os.listdir(input_dir) if file.endswith('.mp3')]
output_files = [file for file in os.listdir(output_dir) if file.endswith('.mp3')]

"""Calculating Disfluency counts for the output merged audio file"""

def calculate_disfluency_counts(file_path):
    model = whisper.load_model("tiny", device="cpu")
    audio = whisper.load_audio(file_path)

    warnings.filterwarnings("ignore", category=UserWarning, module="whisper")
    result = whisper.transcribe(model, audio, language="en", detect_disfluencies=True)
    disfluency_count = 0
    for segment in result["segments"]:
       for word in segment["words"]:
          if "[*]" in word["text"]:
             disfluency_count += 1

    return disfluency_count

input_disfluency_counts = []
output_disfluency_counts = []

for input_file in input_files:
    input_path = os.path.join(input_dir, input_file)
    file_number = input_file.split('_')[1].split('.')[0]
    output_file = f"File_{file_number}_merged.mp3"
    output_path = os.path.join(output_dir, output_file)

    if output_file in output_files:
        for audio_info in audio_files_info:
            if audio_info["file_name"] == input_file:
                dcnt = audio_info["disfluency_count"]
                input_disfluency_counts.append(dcnt)
                break
        output_disfluency_counts.append(calculate_disfluency_counts(output_path))

"""Generating the tips for improving communication skills"""

def get_file_size(file_path):
    return os.path.getsize(file_path) / (1024 * 1024)  # Convert bytes to megabytes

def get_audio_duration(file_path):
    try:
        audio_data, _ = librosa.load(file_path)
        duration_sec = len(audio_data) / _  # Calculate duration based on audio length and sampling rate
        return duration_sec
    except Exception as e:
        print(f"Error loading audio file {file_path}: {e}")
        return None

input_durations_sec = []
output_durations_sec = []
input_sizes_mb = []
output_sizes_mb = []

for input_file in input_files:
    input_path = os.path.join(input_dir, input_file)
    file_number = input_file.split('_')[1].split('.')[0]
    output_file = f"File_{file_number}_merged.mp3"
    output_path = os.path.join(output_dir, output_file)

    if output_file in output_files:
        input_sizes_mb.append(get_file_size(input_path))
        input_durations_sec.append(get_audio_duration(input_path))
        output_sizes_mb.append(get_file_size(output_path))
        output_durations_sec.append(get_audio_duration(output_path))

        if input_durations_sec[-1] is not None and output_durations_sec[-1] is not None:
            print(f"File Name: {input_file}")
            print(f"Input Audio - Size: {input_sizes_mb[-1]:.2f} MB, Duration: {input_durations_sec[-1]:.2f} seconds")
            print(f"Output Audio - Size: {output_sizes_mb[-1]:.2f} MB, Duration: {output_durations_sec[-1]:.2f} seconds")
            print("---------------------------------------------------------------------")
    else:
      print(f"No matching output file found for {input_file}.")

file_counter = 0  # Initialize a counter to track the number of files processed

for audio_info in audio_files_info:
    if file_counter >= 5:  # Exit the loop once the first 5 files are processed
        break

    input_file = audio_info["file_name"]  # Retrieve the input file name from audio_info
    transcription = audio_info["transcription"]
    dcnt = audio_info["disfluency_count"]

    # Prompt for summarization
    prompt = f"""
    Text:
    {transcription}
    disfluency_count:
    {dcnt}
    Suggest only 3 tips as sentences specifically focused on improving communication skills for the speakers based on transcription and disfluency count.
    """
    convo = model.start_chat(history=[])
    convo.send_message(prompt)

    # Print the generated tips
    print(f"Tips for {input_file}:")
    message = convo.last
    if message:
        tip = message.text
        print(f"{tip}")
    else:
        print("No response received")

    file_counter += 1  # Increment the file counter
    print("-----------------------------------------------------------------------------------------------------")

"""Visualizing the input and output files based on duration and disfluency counts"""

def calculate_accuracy(input_disfluency_counts, output_disfluency_counts, tolerance=1):
    total_files = len(input_disfluency_counts)
    correct_predictions = 0

    for input_count, output_count in zip(input_disfluency_counts, output_disfluency_counts):
        if (input_count - output_count) >= tolerance:
            correct_predictions += 1

    accuracy = correct_predictions / total_files
    return accuracy

# Calculate accuracy with a tolerance of 1 (counts within 1 are considered correct)
accuracy = calculate_accuracy(input_disfluency_counts, output_disfluency_counts)
print(f"Accuracy based on disfluency counts: {accuracy * 100:.2f}%")

plt.figure(figsize=(10, 6))

# Plot input data points
plt.plot(file_names, input_durations_sec, 'o-', label='Input Audio', color='blue')
for i, count in enumerate(input_disfluency_counts):
    plt.text(file_names[i], input_durations_sec[i] + 1, f'{count}', ha='center', va='bottom', color='blue')

# Plot output data points
plt.plot(file_names, output_durations_sec, 'o-', label='Output Audio', color='green')
for i, count in enumerate(output_disfluency_counts):
    plt.text(file_names[i], output_durations_sec[i] - 1, f'{count}', ha='center', va='top', color='green')

# Connect input-output pairs with lines
for i in range(len(file_names)):
    plt.plot([file_names[i], file_names[i]], [input_durations_sec[i], output_durations_sec[i]], 'k--')

plt.xlabel('Audio Files')
plt.ylabel('Duration (seconds)')
plt.title('Fluency Analysis based on duration')
plt.legend()
plt.xticks(rotation=45)
plt.grid(False)
plt.tight_layout()